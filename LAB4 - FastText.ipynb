{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanditraore/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset imdb (/Users/nanditraore/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 60.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Callable, Dict, Generator, List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Turn the dataset into a dataset compatible with Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocess(data : dict) -> dict:\n",
    "    '''Takes a document from a dataset, lowers the letters and\n",
    "    replace all punctuations by spaces'''\n",
    "    text = data[\"text\"]\n",
    "    for character in string.punctuation:\n",
    "        if character != \"-\":\n",
    "          text = text.replace(character, ' ')\n",
    "    data[\"text\"] = text.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/nanditraore/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-ab3a7119bbf33038.arrow\n",
      "Loading cached processed dataset at /Users/nanditraore/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-4b4bf0f1f7f30d24.arrow\n",
      "Loading cached processed dataset at /Users/nanditraore/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-ed3318b52d5380e8.arrow\n"
     ]
    }
   ],
   "source": [
    "updated_dataset = dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ft_dataset(dataset, file):\n",
    "    with open(file, 'w') as f:\n",
    "        for d in dataset:\n",
    "            label = '__label__' + str(d['label'])\n",
    "            text = d['text']\n",
    "            f.write(label + ' ' + text + '\\n')\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Train a FastText classifier with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ft_file = to_ft_dataset(updated_dataset[\"train\"], \"ft_train_dataset.txt\")\n",
    "test_ft_file = to_ft_dataset(updated_dataset[\"test\"], \"ft_test_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"ft_train_dataset.txt\", 'r') as f:\n",
    "    train_lines = f.readlines()\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle(train_lines)\n",
    "with open(\"ft_train_dataset.txt\", 'w') as f:\n",
    "    f.writelines(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 6M words\n",
      "Number of words:  96171\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2808106 lr:  0.000000 avg.loss:  0.327647 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = ft.train_supervised(input=\"ft_train_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'accuracy est de: 0.8736 %\n"
     ]
    }
   ],
   "source": [
    "_, p, _ = model.test(test_ft_file)\n",
    "print(\"L'accuracy est de:\", p, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Use the hyperparameters search functionality of FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"ft_train_dataset.txt\", 'r') as f:\n",
    "    data = []\n",
    "    labels = []\n",
    "    for line in f:\n",
    "        label, text = line.split(maxsplit=1)\n",
    "        data.append(text)\n",
    "        labels.append(label)\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Combine the data and labels into the format expected by FastText\n",
    "train = [f\"{label} {text.strip()}\\n\" for label, text in zip(Y_train, X_train)]\n",
    "val = [f\"{label} {text.strip()}\\n\" for label, text in zip(Y_valid, X_valid)]\n",
    "\n",
    "# Save the training and validation data to separate files\n",
    "with open('ft_train_dataset.txt', 'w') as f:\n",
    "    f.writelines(train)\n",
    "with open('ft_valid_dataset.txt', 'w') as f:\n",
    "    f.writelines(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aborting autotune...\n",
      "\n",
      "Training again with best arguments\n",
      "Read 4M words\n",
      "Number of words:  86168\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2352775 lr:  0.000000 avg.loss:  0.163811 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model_hyperparameters = ft.train_supervised(input='ft_train_dataset.txt', autotuneValidationFile='ft_valid_dataset.txt', autotuneDuration=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'accuracy est de: 0.8962 %\n"
     ]
    }
   ],
   "source": [
    "_, p, _ = model_hyperparameters.test(\"ft_valid_dataset.txt\")\n",
    "print(\"L'accuracy est de:\", p, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-  Look at the differences between the default model and the attributes found with hyperparameters search. How do the two models differ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Using the tuned model, take at least 2 wrongly classified examples from the test set, and try explaining why the model failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__0 first off let me say  if you haven t enjoyed a van damme movie since bloodsport  you probably will not like this movie  most of these movies may not have the best plots or best actors but i enjoy these kinds of movies for what they are  this movie is much better than any of the movies the other action guys  segal and dolph  have thought about putting out the past few years  van damme is good in the movie  the movie is only worth watching to van damme fans  it is not as good as wake of death  which i highly recommend to anyone of likes van damme  or in hell but  in my opinion it s worth watching  it has the same type of feel to it as nowhere to run  good fun stuff \n",
      "\n",
      "\n",
      "\n",
      "__label__0 ben   rupert grint   is a deeply unhappy adolescent  the son of his unhappily married parents  his father   nicholas farrell   is a vicar and his mother   laura linney   is     well  let s just say she s a somewhat hypocritical soldier in jesus  army  it s only when he takes a summer job as an assistant to a foul-mouthed  eccentric  once-famous and now-forgotten actress evie walton   julie walters   that he finally finds himself in true  harold and maude  fashion  of course  evie is deeply unhappy herself and it s only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness  br    br   of course it s corny and sentimental and very predictable but it has a hard side to it  too and walters  who could sleep-walk her way through this sort of thing if she wanted  is excellent  it s when she puts the craziness to one side and finds the pathos in the character   like hitting the bottle and throwing up in the sink   that she s at her best  the problem is she s the only interesting character in the film  and it s not because of the script which doesn t do anybody any favours   grint  on the other hand  isn t just unhappy  he s a bit of a bore as well while linney s starched bitch is completely one-dimensional   still  she s got the english accent off pat   the best that can be said for it is that it s mildly enjoyable - with the emphasis on the mildly \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = \"ft_test_dataset.txt\"\n",
    "nb = 0\n",
    "with open(test_data, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if (nb == 2):\n",
    "            break\n",
    "        lab, te = line.split(' ', 1)\n",
    "        lab = lab.split(\"__label__\")[1]\n",
    "        prediction = model_hyperparameters.predict(te.rstrip())[0][0]\n",
    "        prediction = prediction.split(\"__label__\")[1]\n",
    "        if prediction != label:\n",
    "            print(line)\n",
    "            print(\"\\n\")\n",
    "            nb += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Concernant le premier texte, on peut observer que le spectateur a adoré le film, mais qu'il est conscient que ce n'est probablement dû qu'à son admiration envers Van Damme. Malgré les éloges qu'il adresse au film, il indique notamment qu'à moins d'être fan de Van Damme, il y a de fortes chances que le film ne soit pas apprécié. L'avis est donc globalement négatif, malgré un ton assez élogieux, d'où notre évaluation positive.\n",
    "\n",
    "Le second avis, quant à lui, indique clairement que le film n'a pas grand chose de positif, si ce n'est le jeu d'acteur de l'une des actrices. Ici aussi, le fait qu'une grosse partie du texte utilise des termes extrêmement positifs lui fait attribuer une note positive par notre modèle, alors que le ton est assez négatif dans l'ensemble."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
