{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanditraore/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset imdb (/Users/nanditraore/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 60.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Callable, Dict, Generator, List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Turn the dataset into a dataset compatible with Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocess(data : dict) -> dict:\n",
    "    '''Takes a document from a dataset, lowers the letters and\n",
    "    replace all punctuations by spaces'''\n",
    "    text = data[\"text\"]\n",
    "    for character in string.punctuation:\n",
    "        if character != \"-\":\n",
    "          text = text.replace(character, ' ')\n",
    "    data[\"text\"] = text.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/nanditraore/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-ab3a7119bbf33038.arrow\n",
      "Loading cached processed dataset at /Users/nanditraore/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-4b4bf0f1f7f30d24.arrow\n",
      "Loading cached processed dataset at /Users/nanditraore/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-ed3318b52d5380e8.arrow\n"
     ]
    }
   ],
   "source": [
    "updated_dataset = dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ft_dataset(dataset, file):\n",
    "    with open(file, 'w') as f:\n",
    "        for d in dataset:\n",
    "            label = '__label__' + str(d['label'])\n",
    "            text = d['text']\n",
    "            f.write(label + ' ' + text + '\\n')\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Train a FastText classifier with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ft_file = to_ft_dataset(updated_dataset[\"train\"], \"ft_train_dataset.txt\")\n",
    "test_ft_file = to_ft_dataset(updated_dataset[\"test\"], \"ft_test_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"ft_train_dataset.txt\", 'r') as f:\n",
    "    train_lines = f.readlines()\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle(train_lines)\n",
    "with open(\"ft_train_dataset.txt\", 'w') as f:\n",
    "    f.writelines(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 6M words\n",
      "Number of words:  96171\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2808106 lr:  0.000000 avg.loss:  0.327647 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = ft.train_supervised(input=\"ft_train_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'accuracy est de: 0.8736 %\n"
     ]
    }
   ],
   "source": [
    "_, p, _ = model.test(test_ft_file)\n",
    "print(\"L'accuracy est de:\", p, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Use the hyperparameters search functionality of FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"ft_train_dataset.txt\", 'r') as f:\n",
    "    data = []\n",
    "    labels = []\n",
    "    for line in f:\n",
    "        label, text = line.split(maxsplit=1)\n",
    "        data.append(text)\n",
    "        labels.append(label)\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Combine the data and labels into the format expected by FastText\n",
    "train = [f\"{label} {text.strip()}\\n\" for label, text in zip(Y_train, X_train)]\n",
    "val = [f\"{label} {text.strip()}\\n\" for label, text in zip(Y_valid, X_valid)]\n",
    "\n",
    "# Save the training and validation data to separate files\n",
    "with open('ft_train_dataset.txt', 'w') as f:\n",
    "    f.writelines(train)\n",
    "with open('ft_valid_dataset.txt', 'w') as f:\n",
    "    f.writelines(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aborting autotune...\n",
      "\n",
      "Training again with best arguments\n",
      "Read 4M words\n",
      "Number of words:  86168\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2352775 lr:  0.000000 avg.loss:  0.163811 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model_hyperparameters = ft.train_supervised(input='ft_train_dataset.txt', autotuneValidationFile='ft_valid_dataset.txt', autotuneDuration=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'accuracy est de: 0.8962 %\n"
     ]
    }
   ],
   "source": [
    "_, p, _ = model_hyperparameters.test(\"ft_valid_dataset.txt\")\n",
    "print(\"L'accuracy est de:\", p, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-  Look at the differences between the default model and the attributes found with hyperparameters search. How do the two models differ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Using the tuned model, take at least 2 wrongly classified examples from the test set, and try explaining why the model failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m line[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(text)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m!=\u001b[39m label:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_data = \"ft_test_dataset.txt\"\n",
    "nb = 0\n",
    "with open(test_data, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if (nb == 2):\n",
    "            break\n",
    "        line = line.strip().split('\\t')\n",
    "        text = line[0]\n",
    "        label = line[1]\n",
    "        prediction = model.predict(text)[0][0]\n",
    "        if prediction != label:\n",
    "            print(\"Texte:\", text)\n",
    "            print(\"Vrai label:\", label)\n",
    "            print(\"Prédiction:\", prediction)\n",
    "        nb += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
